Literature_DL4TemporalData
**************************************
A paper list about representation learning of temporal data (including multivariate time series, event sequences, and heterogeneous temporal event data). These works could be applied to learn the representations of temporal data for predicitons, in the domains such as healthcare.

.. contents::
    :local:
    :depth: 2

.. sectnum::
    :depth: 2

.. role:: author(emphasis)

.. role:: venue(strong)

.. role:: keyword(emphasis)



Advanced Models for Long Sequences 
===========================

RNN-based Models
-----------------------------------------

`Phased lstm: Accelerating recurrent network training for long or event-based sequences
<https://papers.nips.cc/paper/6310-phased-lstm-accelerating-recurrent-network-training-for-long-or-event-based-sequences.pdf>`_
    | :author:`Neil Daniel, Pfeiffer Michael, Liu Shih-Chii`
    | :venue:`NIPS 2016`
    | :keyword:`Long event-based sequences, Asychronized sparse asynchronous streams, Sparse updating`
    
`Learning the joint representation of heterogeneous temporal events for clinical endpoint prediction
<https://arxiv.org/abs/1803.04837>`_
    | :author:`Liu Luchen, Shen Jianhao, Zhang Ming, Wang Zichang, Tang Jian`
    | :venue:`AAAI 2018`
    | :keyword:`Endpoint prediciton, Sequential Data Modeling, Heterogeneous Event LSTM`
    
`Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks
<https://arxiv.org/abs/1810.09536>`_
    | :author:`Yikang Shen, Shawn Tan, Alessandro Sordoni, Aaron Courville`
    | :venue:`ICLR 2019 (best paper)`
    | :keyword:`NLP, Hidden hierarchically structured, Ordered neurons`

Attention-based Models
-----------------------------------------

`Attention is all you need
<http://papers.nips.cc/paper/7181-attention-is-all-you-need>`_
    | :author:`Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin`
    | :venue:`NIPS 2017`
    | :keyword:`Machine translation, Self attention, Position Encoding`
   

`Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context
<https://arxiv.org/abs/1901.02860>`_
    | :author:`Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov`
    | :venue:`Arxiv`
    | :keyword:`Longer-term temporal dependency, Transformers`

CNN-based Models
-----------------------------------------

`An empirical evaluation of generic convolutional and recurrent networks for sequence modeling
<https://arxiv.org/abs/1803.01271>`_
    | :author:`Shaojie Bai, J. Zico Kolter, Vladlen Koltun`
    | :venue:`Arxiv`
    | :keyword:`NLP, Multivariate timeseries, Temporal Convolution Net`
   


Sequence with Latent Graphs
============================

`Neural Relational Inference for Interacting Systems
<https://arxiv.org/abs/1802.04687>`_
    | :author:`Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, Richard Zemel`
    | :venue:`ICML 2018`
    | :keyword:`Interacting systems, Neural relational inference, Variational auto-encoder`
   



